public class QState : MonoBehaviour
{

    public bool _nWalkable;
    public bool _sWalkable;
    public bool _eWalkable;
    public bool _wWalkable;

    public int _playerUp;
    public int _playerRight;

    public int _idState;

    public QState(bool n, bool s, bool e, bool w, int up, int right, int idState)
    {
        _nWalkable = n;
        _sWalkable = s;
        _eWalkable = e;
        _wWalkable = w;
        _playerUp = up;
        _playerRight = right;
        _idState = idState;
    }
}

public class QTable : MonoBehaviour
{
    // Número de filas y de columnas
    public int _numRows;
    public int _numCols;
    // Tabla con los valores Q
    public float[,] _tablaQ;
    // Lista con los estados
    public QState[] _listaEstados;

    public QTable (WorldInfo world){
        // En las filas se almacena el número de acciones posibles a realizar (N,S,E,O)
        this._numRows = 4;
        // En las columnas se almacenan todos los estados posibles.
        // Para los estados, se han almacenan los vecinos caminables y la posición del enemigo respecto al jugador ( opuestas o iguales )
        this._numCols = 16*9;
        // Inicialización de la tabla Q
        this._tablaQ = new float[this._numRows, this._numCols];
        this._listaEstados = new QState[_numCols];
    }
    
    public void InicializarTabla()
    {
        for(int i=0; i<_numRows; i++)
        {
            for (int j = 0; j < _numCols; j++) 
            {
                _tablaQ[i, j] = 0;          
            }
        }

        InicializarEstados();
    }

    public void InicializarEstados()
    {
        int indice = 0;
        bool n, s, e, w;
        for(int i= 0; i<16; i++)
        {
            for(int j=0; j<3; j++)
            {
                for(int k=0; k<3; k++)
                {
                    if(i/8%2==0)
                    {
                        n = false;
                    }
                    else
                    {
                        n = true;
                    }

                    if(i/4%2==0)
                    {
                        s = false;
                    }
                    else
                    {
                        s = true;
                    }

                    if(i/2%2==0)
                    {
                        e = false;
                    }
                    else
                    {
                        e = true;
                    }
                    if(i%2==0)
                    {
                        w = false;
                    }
                    else
                    {
                        w = true;
                    }

                    _listaEstados[indice] = new QState(n,s,e,w,j,k,indice); 
                    indice++;
                }
            }
        }

    }

    public float DevolverQ(int accion, bool n, bool s, bool e, bool w, int up, int right)
    {
        int indice = 0;
        // Primero se recorre la lista de estados, para identificar de cuál se trata, en base a las posiciones pasadas como parámetro
        for(int i=0; i<_listaEstados.Length; i++)
        {
            if (n == _listaEstados[i]._nWalkable &&
                s == _listaEstados[i]._sWalkable &&
                w == _listaEstados[i]._wWalkable &&
                e == _listaEstados[i]._eWalkable &&
                up == _listaEstados[i]._playerUp &&
                right == _listaEstados[i]._playerRight)
            {
                // Se guarda el índice del estado
                indice = _listaEstados[i]._idState;
            }
        }
        return _tablaQ[accion, indice];
    }

    public int DevolverMejorAccion(bool n, bool s, bool e, bool w, int up, int right)
    {
        int indice = 0;
        // Primero se recorre la lista de estados, para identificar de cuál se trata, en base a las posiciones pasadas como parámetro
        for (int i = 0; i < _listaEstados.Length; i++)
        {
            if (n == _listaEstados[i]._nWalkable &&
                s == _listaEstados[i]._sWalkable &&
                w == _listaEstados[i]._wWalkable &&
                e == _listaEstados[i]._eWalkable &&
                up == _listaEstados[i]._playerUp &&
                right == _listaEstados[i]._playerRight)
            {
                // Se guarda el índice del estado
                indice = _listaEstados[i]._idState;
            }
        }

        int mejorAccion = 0;
        float mejorQ = -1000f;

        for(int i=0; i< _numRows; i++)
        {
            if (_tablaQ[i,indice]>mejorQ)
            {
                mejorAccion = i;
                mejorQ = _tablaQ[i, indice];
            }
        }

        return mejorAccion;
    }

    public float DevolverMejorQ(bool n, bool s, bool e, bool w, int up, int right)
    {
        int indice = 0;
        // Primero se recorre la lista de estados, para identificar de cuál se trata, en base a las posiciones pasadas como parámetro
        for (int i = 0; i < _listaEstados.Length; i++)
        {
            if (n == _listaEstados[i]._nWalkable &&
                s == _listaEstados[i]._sWalkable &&
                w == _listaEstados[i]._wWalkable &&
                e == _listaEstados[i]._eWalkable &&
                up == _listaEstados[i]._playerUp &&
                right == _listaEstados[i]._playerRight)
            {
                // Se guarda el índice del estado
                indice = _listaEstados[i]._idState;
            }
        }

        float mejorQ = -1000f;

        for (int i = 0; i < _numRows; i++)
        {
            if (_tablaQ[i, indice] >= mejorQ)
            {
                mejorQ = _tablaQ[i, indice];
            }
        }

        return mejorQ;
    }

    public void ActualizarQ(int accion, bool n, bool s, bool e, bool w, int up, int right, float actualizedQ)
    {
        int indice = 0;
        // Primero se recorre la lista de estados, para identificar de cuál se trata, en base a las posiciones pasadas como parámetro
        for (int i = 0; i < _listaEstados.Length; i++)
        {
            if (n == _listaEstados[i]._nWalkable &&
                s == _listaEstados[i]._sWalkable &&
                w == _listaEstados[i]._wWalkable &&
                e == _listaEstados[i]._eWalkable &&
                up == _listaEstados[i]._playerUp &&
                right == _listaEstados[i]._playerRight)
            {
                // Se guarda el índice del estado
                indice = _listaEstados[i]._idState;
            }
        }

        // Actualización de la tabla
        _tablaQ[accion,indice] = actualizedQ;
    }
}

public class MyTrainer : IQMindTrainer
{
    public int CurrentEpisode { get; private set; }
    public int CurrentStep { get; private set; }
    public CellInfo AgentPosition { get; private set; }
    public CellInfo OtherPosition { get; private set; }
    public float Return { get; }
    public float ReturnAveraged { get; }
    public event EventHandler OnEpisodeStarted;
    public event EventHandler OnEpisodeFinished;

    private INavigationAlgorithm _navigationAlgorithm;
    private WorldInfo _worldInfo;

    // Tabla Q
    private QTable _QTable;
    // Parámetros del algoritmo
    private QMindTrainerParams _params;

    private int counter = 0;
    private int numEpisode = 0;
    private int accionAnterior = -1;


    public void DoStep(bool train)
    {
        CellInfo agentNextCell;
        CellInfo currentCell = AgentPosition;
        int accion = -1;
        
        do
        {
            if(EscogerNumeroAleatorio())
            {
                accion = AccionAleatoria();
            }
            else
            {
                accion = MejorAccion();
            }

            // Según el código obtenido para la acción, se asigna una nueva posición para el agente
            // 0 - Norte, 1 - Este, 2 - Sur, 3 - Oeste
            agentNextCell = QMind.Utils.MoveAgent(accion, AgentPosition, _worldInfo);
            if(!agentNextCell.Walkable)
            {
                // Penalizar esa acción gravemente, pero sin realizarla para no provocar un error
                float Q = DevolverQ(accion);
                float QNew = ActualizarQ(Q, 0, -10000);
                ActualizarTablaQ(accion, QNew);
            }

        } while(!agentNextCell.Walkable);
        
        // ACTUALIZACIÓN DE LA TABLA Q
        // 1 - Se obtiene la Q del estado actual y la acción escogida
        float currentQ = DevolverQ(accion);
        // 2 - Se obtiene la mejor Q del estado siguiente
        float bestNextQ = DevolverMaxQ(agentNextCell);
        // 3 - Se calcula la recompensa al realizar la acción
        int reward = GetReward(agentNextCell, accion);
        // 4 - Se calcula el nuevo valor de Q aplicando la fórmula
        float actualizedQ = ActualizarQ(currentQ, bestNextQ, reward);
        // 5 - Se actualiza la tabla con el nuevo valor de Q calculado
        ActualizarTablaQ(accion, actualizedQ);

        accionAnterior = accion;

        AgentPosition = agentNextCell;
        CellInfo otherCell = QMind.Utils.MoveOther(_navigationAlgorithm, OtherPosition, AgentPosition);
        OtherPosition = otherCell;

        CurrentStep = counter;
        // En el caso de que el player alcance al agente, o se llegue al máximo de pasos, se finaliza el episodio actual y se comienza uno nuevo
        if(OtherPosition == null || CurrentStep == _params.maxSteps || OtherPosition == AgentPosition)
        {
            OnEpisodeFinished?.Invoke(this, EventArgs.Empty);
            NuevoEpisodio();
        }
        else
        {
            counter += 1;
        }

    }

    private void NuevoEpisodio()
    {
        AgentPosition = _worldInfo.RandomCell();
        OtherPosition = _worldInfo.RandomCell();
        counter = 0;
        CurrentStep = counter;
        numEpisode++;
        CurrentEpisode = numEpisode;
        if(numEpisode%_params.episodesBetweenSaves==0)
        {
            GuardarTablaQ();
        }
        OnEpisodeStarted?.Invoke(this, EventArgs.Empty);
    }

    public void Initialize(QMindTrainerParams qMindTrainerParams, WorldInfo worldInfo, INavigationAlgorithm navigationAlgorithm)
    {
        Debug.Log("QMindTrainer: initialized");

        _navigationAlgorithm = Utils.InitializeNavigationAlgo(navigationAlgorithm, worldInfo);
        _worldInfo = worldInfo;

        AgentPosition = worldInfo.RandomCell();
        OtherPosition = worldInfo.RandomCell();
        OnEpisodeStarted?.Invoke(this, EventArgs.Empty);

        // Almacenas los parámetros
        _params = qMindTrainerParams;
        // Creas e inicializas la tabla Q
        _QTable = new QTable(worldInfo);
        _QTable.InicializarTabla();
       
    }

    private bool EscogerNumeroAleatorio()
    {
        float azar = UnityEngine.Random.Range(0.0f, 1.0f);
        if(azar <= _params.epsilon)
        {
            return true;
        }
        else
        {
            return false;
        }
    }

    private int AccionAleatoria()
    {
        int accion = UnityEngine.Random.Range(0, 4);
        return accion;
    }

    private int MejorAccion()
    {
        // Para escoger la mejor acción, primero debemos averiguar el estado en el que nos encontramos actualmente
        // De esta forma, se escoge la acción que aporte más Q

        int posX = AgentPosition.x;
        int posY = AgentPosition.y;

        CellInfo north = QMind.Utils.MoveAgent(0, AgentPosition, _worldInfo);
        bool n = north.Walkable;

        CellInfo south = QMind.Utils.MoveAgent(2, AgentPosition, _worldInfo);
        bool s = south.Walkable;

        CellInfo east = QMind.Utils.MoveAgent(1, AgentPosition, _worldInfo);
        bool e = east.Walkable;

        CellInfo west = QMind.Utils.MoveAgent(3, AgentPosition, _worldInfo);
        bool w = west.Walkable;

        int up = 0, right = 0;
        if(OtherPosition.x > AgentPosition.x)
        {
            right = 0;
        }
        if(OtherPosition.x < AgentPosition.x)
        {
            right = 1;
        }
        if(OtherPosition.x == AgentPosition.x)
        {
            right = 2;
        }

        if (OtherPosition.y > AgentPosition.y)
        {
            up = 0;
        }
        if (OtherPosition.y < AgentPosition.y)
        {
            up = 1;
        }
        if (OtherPosition.y == AgentPosition.y)
        {
            up = 2;
        }

        return _QTable.DevolverMejorAccion(n,s,e,w,up,right);

    }

    private float DevolverQ (int accion)
    {
        int posX = AgentPosition.x;
        int posY = AgentPosition.y;

        CellInfo north = QMind.Utils.MoveAgent(0, AgentPosition, _worldInfo);
        bool n = north.Walkable;

        CellInfo south = QMind.Utils.MoveAgent(2, AgentPosition, _worldInfo);
        bool s = south.Walkable;

        CellInfo east = QMind.Utils.MoveAgent(1, AgentPosition, _worldInfo);
        bool e = east.Walkable;

        CellInfo west = QMind.Utils.MoveAgent(3, AgentPosition, _worldInfo);
        bool w = west.Walkable;

        int up = 0, right = 0;
        if (OtherPosition.x > AgentPosition.x)
        {
            right = 0;
        }
        if (OtherPosition.x < AgentPosition.x)
        {
            right = 1;
        }
        if (OtherPosition.x == AgentPosition.x)
        {
            right = 2;
        }

        if (OtherPosition.y > AgentPosition.y)
        {
            up = 0;
        }
        if (OtherPosition.y < AgentPosition.y)
        {
            up = 1;
        }
        if (OtherPosition.y == AgentPosition.y)
        {
            up = 2;
        }

        return _QTable.DevolverQ(accion, n,s,e,w,up,right);
    }

    private float DevolverMaxQ(CellInfo nextCell)
    {
        int posX = AgentPosition.x;
        int posY = AgentPosition.y;

        CellInfo north = QMind.Utils.MoveAgent(0, AgentPosition, _worldInfo);
        bool n = north.Walkable;

        CellInfo south = QMind.Utils.MoveAgent(2, AgentPosition, _worldInfo);
        bool s = south.Walkable;

        CellInfo east = QMind.Utils.MoveAgent(1, AgentPosition, _worldInfo);
        bool e = east.Walkable;

        CellInfo west = QMind.Utils.MoveAgent(3, AgentPosition, _worldInfo);
        bool w = west.Walkable;

        int up = 0, right = 0;
        if (OtherPosition.x > AgentPosition.x)
        {
            right = 0;
        }
        if (OtherPosition.x < AgentPosition.x)
        {
            right = 1;
        }
        if (OtherPosition.x == AgentPosition.x)
        {
            right = 2;
        }

        if (OtherPosition.y > AgentPosition.y)
        {
            up = 0;
        }
        if (OtherPosition.y < AgentPosition.y)
        {
            up = 1;
        }
        if (OtherPosition.y == AgentPosition.y)
        {
            up = 2;
        }

        return _QTable.DevolverMejorQ(n, s, e, w, up, right);
    }

    private int GetReward(CellInfo nextCell, int accion)
    {
        int distanciaRealInicial = Mathf.Abs(AgentPosition.x - OtherPosition.x) + Mathf.Abs(AgentPosition.y - OtherPosition.y);
        int distanciaRealFinal = Mathf.Abs(nextCell.x - OtherPosition.x) + Mathf.Abs(nextCell.y - OtherPosition.y);

        int recompensa = 0;
        if(nextCell.x == OtherPosition.x && nextCell.y == OtherPosition.y) { return -100; }
        if(distanciaRealFinal > distanciaRealInicial)
        {
            recompensa += 100;
        }
        else
        {
            if(distanciaRealFinal<=2)
            {
                recompensa -= 100;
            }
           recompensa += -10;
            
        }
        if((nextCell.x == 0 && nextCell.y == 19) ||
                (nextCell.x == 0 && nextCell.y == 0) ||
                (nextCell.x == 19 && nextCell.y == 0) ||
                (nextCell.x == 19 && nextCell.y == 19))
        {
            recompensa -= 1000;
        }
               
        return recompensa;
           
    }

    private float ActualizarQ(float currentQ, float maxNextQ, int reward)
    {
        float actualizedQ = (1 - _params.alpha) * currentQ + _params.alpha * (reward + _params.gamma * maxNextQ);
        return actualizedQ;
    }

    private void ActualizarTablaQ(int accion, float actualizedQ) {

        int posX = AgentPosition.x;
        int posY = AgentPosition.y;

        CellInfo north = QMind.Utils.MoveAgent(0, AgentPosition, _worldInfo);
        bool n = north.Walkable;

        CellInfo south = QMind.Utils.MoveAgent(2, AgentPosition, _worldInfo);
        bool s = south.Walkable;

        CellInfo east = QMind.Utils.MoveAgent(1, AgentPosition, _worldInfo);
        bool e = east.Walkable;

        CellInfo west = QMind.Utils.MoveAgent(3, AgentPosition, _worldInfo);
        bool w = west.Walkable;

        int up = 0, right = 0;
        if (OtherPosition.x > AgentPosition.x)
        {
            right = 0;
        }
        if (OtherPosition.x < AgentPosition.x)
        {
            right = 1;
        }
        if (OtherPosition.x == AgentPosition.x)
        {
            right = 2;
        }

        if (OtherPosition.y > AgentPosition.y)
        {
            up = 0;
        }
        if (OtherPosition.y < AgentPosition.y)
        {
            up = 1;
        }
        if (OtherPosition.y == AgentPosition.y)
        {
            up = 2;
        }

        _QTable.ActualizarQ(accion, n, s, e, w, up, right, actualizedQ);
    }

    private void GuardarTablaQ()
    {
        File.WriteAllLines(@"Assets/Scripts/Grupo8/TablaQ.csv",
            ToCsv(_QTable._tablaQ));
    }

    private static IEnumerable<String> ToCsv<T>(T[,] data, string separator = ";")
    {
        for (int i = 0; i < data.GetLength(0); ++i)
            yield return string.Join(separator, Enumerable
              .Range(0, data.GetLength(1))
              .Select(j => data[i, j]));
    }

}

public class MyTester : IQMind
{
    private WorldInfo _worldInfo;
    // Parámetros para cargar la tabla Q
    // Número de filas y de columnas
    private int _numRows = 4;
    private int _numCols = 16*9;
    // Tabla con los valores Q
    private float[,] _tablaQ;
    // Lista con los estados
    private QState[] _listaEstados = new QState[16*9];

    public void Initialize(WorldInfo worldInfo)
    {
        _worldInfo = worldInfo;
        InicializarEstados();
        LoadQTable();
    }

    public CellInfo GetNextStep(CellInfo currentPosition, CellInfo otherPosition)
    {
        QState state = CalculateState(currentPosition, otherPosition);
        CellInfo agentCell = null;
        
            int action = GetAction(state);
            agentCell = QMind.Utils.MoveAgent(action, currentPosition, _worldInfo);
            Debug.Log("Action = " + action);
        if(!agentCell.Walkable)
        {
            CalculateState(currentPosition, otherPosition);
        }
        Debug.Log(currentPosition.x.ToString() + "" + currentPosition.y.ToString());    

        return agentCell;
    }

    private void InicializarEstados()
    {
        int indice = 0;
        bool n, s, e, w;
        for (int i = 0; i < 16; i++)
        {
            for (int j = 0; j < 3; j++)
            {
                for (int k = 0; k < 3; k++)
                {
                    if (i / 8 % 2 == 0)
                    {
                        n = false;
                    }
                    else
                    {
                        n = true;
                    }

                    if (i / 4 % 2 == 0)
                    {
                        s = false;
                    }
                    else
                    {
                        s = true;
                    }

                    if (i / 2 % 2 == 0)
                    {
                        e = false;
                    }
                    else
                    {
                        e = true;
                    }
                    if (i % 2 == 0)
                    {
                        w = false;
                    }
                    else
                    {
                        w = true;
                    }

                    _listaEstados[indice] = new QState(n, s, e, w, j, k, indice);
                    indice++;
                }
            }
        }
    }

    private int GetAction(QState state)
    {
        // Usar tabla Q aprendida para seleccionar la mejor acción.
        int mejorAccion = 0;
        float mejorQ = -1000f;

        for (int i = 0; i < _numRows; i++)
        {
            if (_tablaQ[i, state._idState] >= mejorQ)
            {
                mejorAccion = i;
                mejorQ = _tablaQ[i, state._idState];
            }
        }

        return mejorAccion;
    }

    // Función para cargar los datos de la tabla Q conseguida tras el entrenamiento
    private void LoadQTable()
    {
        string filePath = @"Assets/Scripts/Grupo8/TablaQ.csv";
        StreamReader reader;
        if(File.Exists(filePath)) {
            reader = new StreamReader(File.OpenRead(filePath));
            _tablaQ = new float[_numRows, _numCols];
            int contador = 0;
            while(!reader.EndOfStream && contador<_numRows)
            {
                var line = reader.ReadLine();
                var values = line.Split(';');
                for(int i=0; i<values.Length; i++)
                {
                    _tablaQ[contador, i] = (float)Convert.ToDouble(values[i]);
                }
                contador++;
            }
        }
    }

    private QState CalculateState(CellInfo currentPosition, CellInfo otherPosition)
    {
        int posX = currentPosition.x;
        int posY = currentPosition.y;

        CellInfo north = QMind.Utils.MoveAgent(0, currentPosition, _worldInfo);
        bool n = north.Walkable;

        CellInfo south = QMind.Utils.MoveAgent(2, currentPosition, _worldInfo);
        bool s = south.Walkable;

        CellInfo east = QMind.Utils.MoveAgent(1, currentPosition, _worldInfo);
        bool e = east.Walkable;

        CellInfo west = QMind.Utils.MoveAgent(3, currentPosition, _worldInfo);
        bool w = west.Walkable;

        int up = 0, right = 0;
        if (otherPosition.x > currentPosition.x)
        {
            right = 0;
        }
        if (otherPosition.x < currentPosition.x)
        {
            right = 1;
        }
        if (otherPosition.x == currentPosition.x)
        {
            right = 2;
        }

        if (otherPosition.y > currentPosition.y)
        {
            up = 0;
        }
        if (otherPosition.y < currentPosition.y)
        {
            up = 1;
        }
        if (otherPosition.y == currentPosition.y)
        {
            up = 2;
        }


        for (int i = 0; i < _listaEstados.Length; i++)
        {
            if (n == _listaEstados[i]._nWalkable &&
                s == _listaEstados[i]._sWalkable &&
                w == _listaEstados[i]._wWalkable &&
                e == _listaEstados[i]._eWalkable &&
                up == _listaEstados[i]._playerUp &&
                right == _listaEstados[i]._playerRight)
            {
                // Se devuelve el estado
                return _listaEstados[i];
            }
        }
        return null;
    }
}